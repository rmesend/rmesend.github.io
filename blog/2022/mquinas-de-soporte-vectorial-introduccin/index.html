<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Máquinas de Soporte Vectorial</h3> <h4>Una introducción a métodos supervisados.</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TvdPZoOug6Rr5gFb8AzX8g.png"><figcaption>Imagen multiespectral tomada con cámara Altum, banda redEdge(717nm).</figcaption></figure> <p>La técnica de máquinas de soporte vectorial había sido por mucho tiempo el frente del caballo de batalla de el <em>machine learning</em>. Luego se extendió al campo del <em>data mining</em> y en conjunto con otras técnicas forman la ciencia de datos. El deep learning llegó a sustituir está técnica posteriormente.</p> <p>Vladimir Vapnik es el proponente del método, trabajó en AT&amp;T Bell Labs donde hace su propuesta para hacer regresión y clasificación con máquinas de soportes, fue muy popular y actualmente ya ha sido superado. Sin embargo, aún es una buena solución para algunos casos de estudio.</p> <p>Inician con una idea que se conocía en su momento, está técnica se llama Análisis Discriminante. Esto quiere decir que se busca encontrar un hiperplano que supere a las clases con el mayor margen posible (Ver figura1).</p> <p>A nivel teórico todo MSV(Máquinas de Soporte Vectorial) es construido en la idea de que solamente existen dos categorías(por facilidad matemática), sin embargo, esta técnica funciona para n categorías.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/957/1*u7tfZE70TD_j4Otj_UcF4w.jpeg"><figcaption>Figura1: Hiperplano de separación entre categorías.</figcaption></figure> <p>Una de las cosas que agrega este método, es que va encontrar un plano de separación con margen máximo. Se utiliza el principio de cercanía invertido. En este caso tenemos un margen que divide las categorías.</p> <p>Lo que se busca es encontrar la ecuación de un hiperplano, que divida las clases y tenga un margen máximo. Después esa ecuación se utiliza para clasificar los nuevos elementos dependiendo de dónde se ubiquen dentro del plano.</p> <h4>¿Cómo funciona?</h4> <p>Usando 2 puntos se pueden calcular y=mx + b y así con los puntos en el plano poder calcular la formula de la pendiente (m=(Y2-Y1)/(X2-X1)) y luego b para calcular la <strong>recta de separación (figura 2)</strong>. Luego se hacen traslaciones sobre las rectas de margen. Luego se puede extraer el hiperplanos no óptimos. En esos casos se busca el hiperplano con error mínimo. Esto es importante recalcar, recordemos que estamos haciendo un modelado y en la realidad pocas veces tenemos una predicción perfecta.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/454/1*LDT5S2ii2l5WBIx3S7bqnQ.png"><figcaption>figura 2: Ecuación a resolver. <a href="https://www.researchgate.net/figure/A-linear-Support-Vector-Machine-SVM-case_fig2_332777280" rel="external nofollow noopener" target="_blank">https://www.researchgate.net/figure/A-linear-Support-Vector-Machine-SVM-case_fig2_332777280</a></figcaption></figure> <p>Esta función discriminante lineal podemos tener muchas, sin embargo siempre se debe de tener el margen máximo y que se equivoque lo mínimo posible (figura 3).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*z1jbwxnuPYzV733gx0fr4g.gif"><figcaption>Figura 3: Búsqueda del plano.</figcaption></figure> <h4>¿Qué ocurre cuando los datos no son linealmente separables?</h4> <p>Cuando los datos están con esta condición, se pueden utilizar una transformación (proyección) con una función núcleo o <em>Kernel</em> (a esto también se conoce como el truco del núcleo) para transformar los datos y así poder separarlos.</p> <p>En Python y R se tienen tres o cuatro posibles núcleos (figura 3), inclusive si fuera requerido se puede programar un kernel con el teorema de Marcer. Este da condiciones necesarias y suficientes para tener un f sea un núcleo o kernel.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/962/1*F_JAIzxttZgSuOf6mLA0zg.png"><figcaption>Figura3 Kernel Python <a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" rel="external nofollow noopener" target="_blank">https://scikit-learn.org/stable/modules/svm.html#svm-kernels</a></figcaption></figure> <h4>Vamos al código:</h4> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cea264201e6df13fbc65f34db0d46481/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/cea264201e6df13fbc65f34db0d46481/href</a></iframe> <p>Se puede revisar el proyecto con los datos para el ejemplo aquí:</p> <p><a href="https://github.com/rmesend/SVM/blob/main/SVM%20PIPELINES.ipynb" rel="external nofollow noopener" target="_blank">SVM/SVM PIPELINES.ipynb at main · rmesend/SVM</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6fd908563a4" width="1" height="1" alt=""></p> </body></html>